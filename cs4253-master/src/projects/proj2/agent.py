#!/usr/bin/env python3

from ...lib.game import Agent, RandomAgent, GreedyAgent
from ...lib import ddqn
import math, random
import numpy as np

dqn = True


class MinimaxAgent(Agent):
	"""An agent that makes decisions using the Minimax algorithm, using a
	evaluation function to approximately guess how good certain states
	are when looking far into the future.

	:param evaluation_function: The function used to make evaluate a
		GameState. Should have the parameters (state, player_id) where
		`state` is the GameState and `player_id` is the ID of the
		player to calculate the expected payoff for.

	:param alpha_beta_pruning: True if you would like to use
		alpha-beta pruning.

	:param max_depth: The maximum depth to search using the minimax
		algorithm, before using estimates generated by the evaluation
		function.
	"""

	def __init__(self, evaluate_function, alpha_beta_pruning=False, max_depth=5):
		super().__init__()
		self.evaluate = evaluate_function
		self.alpha_beta_pruning = alpha_beta_pruning
		self.max_depth = max_depth

		if dqn:
			self.state_size = 6
			self.action_size = 100
			print(self.state_size, self.action_size)
			self.dqnAgent = ddqn.DQNAgent(self.state_size, self.action_size)

		#from pathlib import Path

		#my_file = Path("/path/to/file")
		#if my_file.is_file():
		#	self.dqnAgent.load("./save/discrete_soccer.h5")

		else:
			self.dqnAgent = None

	def decide(self, state):
		# TODO: Implement this agent!
		#
		# Read the documentation in /src/lib/game/_game.py for
		# information on what the decide function does.
		#
		# Do NOT call the soccer evaluation function that you write
		# directly from this function! Instead, use
		# `self.evaluate`. It will behave identically, but will be
		# able to work for multiple games.
		#
		# Do NOT call any SoccerState-specific functions! Assume that
		# you can only see the functions provided in the GameState
		# class.
		#
		# If you would like to see some example agents, check out
		# `/src/lib/game/_agents.py`.
		if not self.alpha_beta_pruning:
			# return self.maxValue(state, state.current_player, 0)
			return self.minimax(state, state.current_player)
		else:
			return self.minimax_with_ab_pruning(state, state.current_player)

	def maxValue(self, state, player, depth):
		if depth > self.max_depth or state.is_terminal:
			return self.evaluate(state, player, self)

		v = math.inf * -1
		for a in state.actions:
			newState = state.act(a)
			if not newState:
				continue
			v = max(v, self.minValue(newState, player, depth + 1))
		return v

	def minValue(self, state, player, depth):
		if depth > self.max_depth or state.is_terminal:
			return self.evaluate(state, player, self)
		v = math.inf

		for a in state.actions:
			newState = state.act(a)
			if not newState:
				continue
			v = min(v, self.maxValue(newState, player, depth + 1))
		return v

	def minimax(self, state, player, depth=1):
		# This is the suggested method you use to do minimax.  Assume
		# `state` is the current state, `player` is the player that
		# the agent is representing (NOT the current player in
		# `state`!)  and `depth` is the current depth of recursion.

		# return super().decide(state)
		bestVal = math.inf * -1
		best = None
		for a in state.actions:
			newState = state.act(a)
			if not newState:
				continue
			u = self.maxValue(newState, state.current_player, 0)
			if u > bestVal:
				bestVal = u
				best = a

		print("Best action for ", player, "is", best, "has a val of", bestVal)
		return best

	def maxValueAB(self, state, player, depth=1, a=float('inf'), b=-float('inf')):
		if depth > self.max_depth or state.is_terminal:
			return self.evaluate(state, player, self)
		v = -math.inf
		for ac in state.actions:
			newState = state.act(ac)
			if not newState:
				continue
			v = max(v, self.minValueAB(newState, player, depth + 1, a, b))
			if v >= b:
				return v
			a = max(a, v)

		return v

	def minValueAB(self, state, player, depth=1, a=float('inf'), b=-float('inf')):
		if depth > self.max_depth or state.is_terminal:
			return self.evaluate(state, player, self)

		v = math.inf
		for ac in state.actions:
			newState = state.act(ac)
			if not newState:
				continue
			v = min(v, self.maxValueAB(newState, player, depth + 1, a, b))
			if v <= a:
				return v
			b = min(b, v)

		return v

	def minimax_with_ab_pruning(self, state, player, depth=1,
								alpha=float('inf'), beta=-float('inf')):
		bestVal = -math.inf
		for a in state.actions:
			newState = state.act(a)
			if not newState:
				continue
			u = self.maxValueAB(newState, player, 0, alpha, beta)
			if u > bestVal:
				bestVal = u
				best = a

		print("Best action for ", player, "is", best, "has a val of", bestVal)
		return best

	def learn(self, states, player_id):
		if states[len(states) - 1].winner is None:
			print("Draw")
		elif states[len(states) - 1].winner == player_id:
			print("Win")
		else:
			print("Loss")
		#for a in states:
		#	print(a.winner)

		if self.dqnAgent is not None:
			self.dqnAgent.update_target_model()

	def predictMinmax(self, state, player_id):
		if self is None:
			#print("Self is None")
			return random.randrange(0, 10)

		if self.dqnAgent is None:
			#print("dqnAgent is None")
			return random.randrange(0, 10)

		npState = self.RnpState(state)
		a = np.random.rand()
		if a <= self.dqnAgent.epsilon:
			#print("epsilon rand:", a, "<=", self.dqnAgent.epsilon)
			return random.randrange(0, 100)

		act_val = self.dqnAgent.model.predict(npState)
		a = np.argmax(act_val[0])
		print("Predicted val: ", a)
		return a

	# npState = np.reshape(npState, )

	def RnpState(self, state):
		intState = [state.players[0]['x'], state.players[0]['y'],
				   state.players[1]['x'], state.players[1]['y'],
				   state.ball['x'], state.ball['y']]
		#intState = np.array(intState)
		intState = np.reshape(intState, [1, self.state_size])
		#print("Shape:", intState.shape)
		return intState

	#def hintReward(self, state):


	# Unchanged
	'''def __sigmoid(self, x):
		return 1 / (1 + np.exp(-x))

	def __sigmoid_derivative(self, x):
		return x * (1 - x)

	def predictL(self, inputs):
		return self.__sigmoid(np.dot(inputs, self.synaptic_weights))

	def train(self, input, output, numOfIters):
		for iteration in range(0, numOfIters):
			out = self.predict(input)
			error = output - out
			adjust = np.dot(input.T, error * self.__sigmoid_derivative(out))
			self.synaptic_weights += adjust'''
